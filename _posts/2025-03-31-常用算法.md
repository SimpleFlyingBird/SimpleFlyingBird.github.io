---
layout:     post
title:      常用算法
subtitle:   常用深度学习和刷题算法
date:       2025-03-31
author:     yinzp
header-img: img/v2-b2a18c2484964f8ebb4aa7fcd316e048_r.jpg
catalog: true
tags:
    - 算法
    - 深度学习
---
# 深度学习
## 搭建神经网络
```python
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        # new_w = [(w + 2padding) - k] / stride + 1
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)

        self.fc1 = nn.Linear(32 * 8 * 8, 10)

    def forward(self, x):
        # 输入x的形状: [batch_size, 3, 32, 32] -> [batch_size, 16, 32, 32] -> [batch_size, 16, 16, 16] 
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        # [batch_size, 16, 16, 16] -> [batch_size, 32, 16, 16] -> [batch_size, 32, 8, 8]
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        # [batch_size, 32*8*8] -> [batch_size, 10]
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
```
## 激活函数
每个样本仅仅预测其为前景类的概率，所以每个样本仅仅输出一个值，通过`sigmoid`将其转换为概率
```python
import numpy as np
# sigmoid
def sigmoid(inputs: np.ndarray) -> np.ndarray:
	"""
	inputs: [num_sample]
	"""
	s = 1 / (1 + np.exp(-inputs))
	return s
```
每个样本预测其为多个类别的概率，需要通过`softmax`将其转换为类别概率，再通过labels取出它为真值的概率。
```python
# softmax函数
def softmax(inputs: np.ndarray) -> np.ndarray:
	"""
	inputs: [num_sample, num_cls]
		sample0: [cls0, cls1, cls2, cls3]
		sample1: ...
	"""
	exp_inputs = np.exp(inputs)	# [num_sample, num_cls]
	# [num_sample, num_cls]第一维度求和，结果为 [num_sample,]，但是keepdims，所以结果为[num_sample, 1]
	exp_sum = np.sum(exp_inputs, axis=1, keepdims=True)
	# [num_sample, num_cls] / [num_sample, 1]
	s = exp_inputs / exp_sum
	return s
```
## 后处理
### NMS
1. 先将各个目标框根据置信度从大到小排序
2. 依次挑出一个框，和其余框计算IoU，将IoU超过阈值的其余框删除

```python
import numpy as np
from typing import List
class NMS:
	def __init__(self, iou_thresh=0.5):
		self.iou_thresh = iou_thresh
		
	@static_method
	def cal_iou(box1: np.ndarray, box2: np.ndarray) -> float:
		"""
		box: [x1, y1, x2, y2]
		"""
		# 计算相交区域面积
		inter_lefttop_x = max(box1[0], box2[0])
		inter_lefttop_y = max(box1[1], box2[1])
		inter_rightbottom_x = min(box1[2], box2[2])
		inter_rightbottom_y = min(box1[3], box2[3])
		inter_area = max(0, inter_rightbottom_x - inter_lefttop_x) * max(0, inter_rightbottom_y - inter_lefttop_y)
		# 计算各自面积
		area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
		area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
		iou = inter_area / float(area_box1 + area_box2 - inter_area + 1e-10)
		return iou
	
	def process(bbox_list: np.ndarray) -> List[int]:
		"""
		bbox_list: [num_bbox, 5] for x0, y0, x1, y1, score
		"""
		obj_scores = bbox_list[:, -1]
		# np.argsort返回的是索引
		sorted_idx = np.argsort(obj_scores)[::-1]
		keep_idx: List[int] = []
		while sorted_idx:
			# 取出剩余目标中分数最高的框
			best_bbox_idx = sorted_idx[0]
			keep_idx.append(best_bbox_idx)
			if len(sorted_idx) == 1:
				break
			# 计算分数最高的框和其余框的IoU
			iou_list = np.zeros(shape=(len(sorted_idx)-1))
			for i in range(iou_list.shape[0]):
				iou_list[i] = NMS.cal_iou(bbox_list[best_bbox_idx], bbox_list[sorted_idx[i+1]])
			# np.where返回 (索引array, )
			overlap_bbox_idx = np.where( iou_list > self.iou_thresh)[0] + 1
			sorted_idx = np.delete(sorted_idx, overlap_bbox_idx)
			sorted_idx = np.delete(sorted_idx, 0)
			
		return keep_idx
			
if __name__ == "__main__":
	# 单类别应用NMS
    # np.array()  创建numpy数组
    boxes = np.array([[10, 10, 40, 40, 0.9], [11, 12, 43, 43, 0.7], [9, 9, 39, 38,  0.8]])  # [xmin, ymin, xmax, ymax]
    iou_thresh = 0.1   # iou阈值
 	nms_handler = NMS(iou_thresh=iou_thresh)
    # 应用NMS
    indices_to_keep = nms_handler.process(boxes)
    print("保留的边界框索引:", indices_to_keep)
```
# 常用损失
## 分类损失
交叉熵
```python
import numpy as np
# 模型预测的都是该样本为前景类的分数
predicted_cls = np.array([0.3, 0.5, 0.7, 0.4])
labels = np.array([0, 1, 1, 0])
# 前景类损失
bce_loss_positive = -(labels*np.log(predicted_cls))
# 背景类损失
bce_loss_negative = -((1-labels)np.log((1-predicted_cls)))
bce_loss = (bce_loss_positive + bce_loss_negative) / labels.shape[0]
```
## 回归损失
均方差

```python
import numpy as np
def MSE(predict, labels):
	diff = (predict - labels)**2
	mse_loss = np.sum(diff) / labels.shape[0]
	return mse_loss
```

# leetcode
参考[算法通关手册](https://algo.itcharge.cn/)
## 排序
### 选择排序
维护一个指针指向左侧已经排好序的末端，依次将右侧未排序部分中最小的数找出来放置到排好序的末端
```python
def selection_sort(inputs_list: List[int]) -> List[int]:
	finish_point = 0
	while finish_point < len(inputs_list):
		min_ind = finish_point
		for process_point in range(finish_point+1, len(inputs_list)):
			if inputs_list[process_point] < inputs_list[min_ind]:
				min_ind = process_point
		inputs_list[min_ind], inputs_list[finish_point] = inputs_list[finish_point], inputs_list[min_ind]
		finish_point += 1
	return inputs_list
```
### 冒泡排序
一直从左往右比较，将较大值交换置右边，直至一次遍历没交换

```python
def bubble_sort(inputs_list: List[int]) -> List[int]:
	length = len(inputs_list)
	swap = True
	while swap:
		swap = False
		for i in range(0, length-1):
			if inputs_list[i] > inputs_list[i+1]:
				inputs_list[i], inputs_list[i+1] = inputs_list[i+1], inputs_list[i]
				swap = True
	return inputs_list			
```
### 归并排序
![归并排序](https://i-blog.csdnimg.cn/direct/4be1a18c44fd4b18808c480f7d52cb15.png)

```python
def merge_sort(arr):
    """归并排序"""
    if len(arr) == 1:
        return arr
    # 使用二分法将数列分两个
    mid = len(arr) // 2
    left = arr[:mid]
    right = arr[mid:]
    # 使用递归运算
    return marge(merge_sort(left), merge_sort(right))


def marge(left, right):
    """排序合并两个数列"""
    result = []
    # 两个数列都有值
    while len(left) > 0 and len(right) > 0:
        # 左右两个数列第一个最小放前面
        if left[0] <= right[0]:
            result.append(left.pop(0))
        else:
            result.append(right.pop(0))
    # 只有一个数列中还有值，直接添加
    result += left
    result += right
    return result

merge_sort([11, 99, 33 , 69, 77, 88, 55, 11, 33, 36,39, 66, 44, 22])

# 返回结果[11, 11, 22, 33, 33, 36, 39, 44, 55, 66, 69, 77, 88, 99]
```

## 树结构
### 递归遍历

```python

```

## 图结构
### 深度优先搜索 
1. 堆栈实现
栈实践的关键是：每次将一个节点的所有子节点推入栈后，处理其中一个子节点，将该子节点的所有子节点再推入栈中。
```python
from typing import Dict, List


def graph_dfs(graph: Dict[str, List[str]], start_node: str) -> None:
  # 记录是否已经访问过的节点
  visited = set()
  # 处理栈
  stack = [start_node]
  while stack:
    # 推出栈尾节点
    current_node = stack.pop()
    # 将该节点的子节点推入，反向推入，从左往右处理
    stack.extend(graph[current_node][::-1])
    if current_node not in visited:
      print(current_node)
      visited.add(current_node)

# 示例用法
graph = {
    'A': ['B', 'C'],
    'B': ['D', 'E'],
    'C': ['F'],
    'D': [],
    'E': ['F'],
    'F': []
}

graph_dfs(graph, 'A')
"""
输出
A
B
D
E
F
C
"""
```
2. 递归
递归方法实现
```python
def dfs_recursive(graph, vertex, visited):
    if vertex not in visited:
        print(vertex)
        visited.add(vertex)
        # 此处可有许多变体，假如是网格状的可以遍历上下左右
        for neighbor in graph[vertex]:
            dfs_recursive(graph, neighbor, visited)

# 示例用法
graph = {
    'A': ['B', 'C'],
    'B': ['D', 'E'],
    'C': ['F'],
    'D': [],
    'E': ['F'],
    'F': []
}

dfs_recursive(graph, 'A', set())
```
### 广度优先搜索
广度优先搜索靠`collections.queue`实现，左侧popleft出父节点进行处理，右侧则append进该父节点的所有子节点预备处理。
例子：最大岛屿面积
```python
def max_island_area(island: List[List[str]]) -> int:
	row_num = len(island)
	col_num = len(island[0])
	max_area = 0
	for i in range(row_num):
		for j in range(col_num):
		temp_ans = 0
		if island[i][j] == "1":
			# 发现岛屿，对该岛屿进行广度优先搜索
			process_queue = collections.queue([(i, j)])
			island[i][j] = 0
			temp_ans += 1
			while len(process_queue) != 0:
				cur_i, cur_j = process_queue.popleft()
				## 四个搜索方向
				search_direction = [(1, 1), (1, -1), (-1, 1), (-1, -1)]
				for dir in search_direction:
					next_i = cur_i + dir[0]
					next_j = cur_j + dir[1]
					if 0 <= next_i < row_num and 0 <= next_j < col_num and island[next_i][next_j] == "1":
						process_queue.append((next_i, next_j))
						island[next_i][next_j] = "0"
						temp_ans += 1
			max_area = max(max_area, temp_ans)
	return max_area
```
