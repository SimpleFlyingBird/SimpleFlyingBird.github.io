---
layout:     post
title:      语言模型
subtitle:   语言模型相关知识点
date:       2025-04-02
author:     yinzp
header-img: img/v2-b2a18c2484964f8ebb4aa7fcd316e048_r.jpg
catalog: true
tags:
    - LLM
    - 预训练
---

闭源模型
|模型名称|参数量|上下文窗口|备注|
| ---- | ---- | ---- | ---- |
|GPT - 4 Turbo|~1.8T|128k tokens|OpenAI旗舰模型|
|Claude 3 Opus|~200B|200k tokens|Anthropic最强版本|
|Gemini Ultra|~1.2T|32k tokens|Google多模态模型|
|GPT - 3.5|175B|16k tokens|OpenAI经典模型|
|PaLM 2|340B|32k tokens|Google新一代模型|

开源模型
|模型名称|参数量|上下文窗口|架构特点|
| ---- | ---- | ---- | ---- |
|LLaMA - 2 70B|70B|4k - 32k*|Meta官方版4k，微调可扩展|
|Mixtral 8x7B|46.7B*|32k tokens|8个专家模型混合|
|Mistral 7B|7B|32k tokens|小体量高性能|
|Falcon 180B|180B|2k tokens|TII超大模型|
|Qwen - 72B|72B|32k tokens|阿里云通义千问|
|GLM - 4|130B|128k tokens|智谱AI支持超长文本|

# 模型训练

## tokenization的训练

## 预训练

