---
layout:     post
title:      对话大模型
subtitle:   对话大模型上下文管理和长文本问题
date:       2025-03-31
author:     yinzp
header-img: img/v2-b2a18c2484964f8ebb4aa7fcd316e048_r.jpg
catalog: true
tags:
    - 大模型
    - 对话大模型
---
对话应用在大模型结构设计上和其他大模型区别不大，需要额外的应用支持上下文的管理。

# 上下文管理
## 滑动窗口
只保留最近的K轮对话，避免上下文过长。
```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=2)  # 保留最近 2 轮
memory.save_context({"input": "第一句"}, {"output": "回复1"})
memory.save_context({"input": "第二句"}, {"output": "回复2"})
memory.load_memory_variables({})  # 返回最近 2 轮对话
```
## 摘要记忆
利用LLM生成对话的摘要，替代原始记忆。
```python
from langchain.memory import ConversationSummaryMemory
from langchain.llms import OpenAI

memory = ConversationSummaryMemory(llm=OpenAI())
memory.save_context({"input": "用户说了很多话..."}, {"output": "AI 的回复..."})
memory.load_memory_variables({})  # 返回摘要，而非原始对话
```

## 实体记忆
通过LLM辅助提取记忆关键实体（如人名或日期）
```python
from langchain.memory import EntityMemory

memory = EntityMemory(llm=OpenAI())
memory.save_context({"input": "我叫张三"}, {"output": "好的，张三！"})
memory.load_memory_variables({"input": "我是谁？"})  # 返回：{'entities': {'张三': '用户的名字是张三'}}
```


## 向量存储记忆
在 LangChain 中，向量存储记忆（VectorStore-Backed Memory） 是一种将对话历史存储在向量数据库中，并通过语义检索（而非简单关键词匹配）来回忆相关上下文的记忆机制。它特别适合长对话或需要从历史中精准检索片段的应用（如客服、知识库问答）。
工作流程：
1. 向量化存储：将对话文本通过嵌入模型（如 OpenAI Embeddings）转换为向量，存入**向量数据库**（如 FAISS、Chroma）。
2. 语义检索：当新输入到来时，计算其向量与历史向量的相似度，返回最相关的历史片段。
3. 动态更新：每次对话后，将新内容实时添加到向量库。

```python
from langchain.memory import VectorStoreRetrieverMemory
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts([""], embeddings)
memory = VectorStoreRetrieverMemory(retriever=vectorstore.as_retriever())

memory.save_context({"input": "巴黎是法国的首都"}, {"output": "是的！"})
memory.load_memory_variables({"input": "法国的首都是哪？"})  # 返回相关历史
```

## 混合使用记忆

```python
from langchain.memory import (
    ConversationBufferWindowMemory,
    ConversationSummaryMemory,
    VectorStoreRetrieverMemory,
    CombinedMemory
)
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI

# 1. 滑动窗口记忆（保留最近3轮）
buffer_memory = ConversationBufferWindowMemory(k=3, memory_key="recent_chat")

# 2. 摘要记忆（用LLM压缩历史）
summary_memory = ConversationSummaryMemory(
    llm=OpenAI(),
    memory_key="summary"
)

# 3. 向量存储记忆（FAISS）
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts([""], embeddings)
vector_memory = VectorStoreRetrieverMemory(
    retriever=vectorstore.as_retriever(),
    memory_key="vector_memory"
)

# 组合所有记忆
combined_memory = CombinedMemory(
    memories=[buffer_memory, summary_memory, vector_memory]
)

# 在链中使用
conversation = ConversationChain(
    llm=OpenAI(),
    memory=combined_memory,
    verbose=True
)

# 模拟长对话
conversation.predict(input="什么是LangChain？")
conversation.predict(input="它有哪些核心模块？")
conversation.predict(input="请用中文解释记忆模块")
conversation.predict(input="我们之前讨论过什么？")  # 触发summary和vector_memory
```

# 多Agent协作

# 长度外推（Length Extrapolation）
**train on short, test on long**

当大模型在训练时使用的 Token 序列长度较短（如 2K/4K），但推理时需要处理 更长的输入序列（如 32K/100K）时，会面临严重的 长度外推（Length Extrapolation）问题，导致模型性能下降（如丢失上下文、生成质量降低）。


Position Interpolation(PI)：
假设我们要将RoPE - based LLM的context length从 $L$ 扩展到 $tL$ ，那么就相应将位置索引缩放为原来的 $1/t$ ，就是将公式(1)中的映射改为:

$$
f_t^{\text{PI}}(x,m,\theta)=f\left(x,\frac{m}{t},\theta\right).
$$

这样经过比较少步数的contiual pretraining就可以很快收敛，从而将context length扩展到 $tL$
